{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MODEL2_optimization.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DXqUrN5XdEkh","colab_type":"text"},"source":["Code produced on google colab environment"]},{"cell_type":"code","metadata":{"id":"WJViH6qw2Bc6","colab_type":"code","outputId":"f75dbf7b-9296-4aff-dc2b-3ad078105234","executionInfo":{"status":"ok","timestamp":1573141306866,"user_tz":0,"elapsed":26677,"user":{"displayName":"Nuno Carlos","photoUrl":"","userId":"16472806472979258588"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GXJCG-ki2cFZ","colab_type":"code","outputId":"3d4ff1a2-064b-4ee3-f096-a9aea67204b6","executionInfo":{"status":"ok","timestamp":1573141310003,"user_tz":0,"elapsed":8174,"user":{"displayName":"Nuno Carlos","photoUrl":"","userId":"16472806472979258588"}},"colab":{"base_uri":"https://localhost:8080/","height":62}},"source":["## Import all packages needed\n","\n","import pandas as pd \n","import cv2                 \n","import numpy as np         \n","import os                  \n","from random import shuffle\n","from tqdm import tqdm  \n","import scipy\n","import skimage\n","from skimage.transform import resize\n","import matplotlib.pyplot as plt\n","import tensorflow as tf"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"SlbQGIi6dLYe","colab_type":"text"},"source":["Import train, validation arrays"]},{"cell_type":"code","metadata":{"id":"nPqMx2Hf2gg3","colab_type":"code","colab":{}},"source":["X_train=np.load('/content/drive/My Drive/identify_pneumonia-master/colab_Data/x_train_200.npy')\n","y_train=np.load('/content/drive/My Drive/identify_pneumonia-master/colab_Data/y_train_200.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6o3uEsJ2jlG","colab_type":"code","colab":{}},"source":["X_train=X_train.reshape(6500,200,200,3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6QOIL6j2pDa","colab_type":"code","colab":{}},"source":["X_val=np.load('/content/drive/My Drive/identify_pneumonia-master/colab_Data/x_val_200.npy')\n","y_val=np.load('/content/drive/My Drive/identify_pneumonia-master/colab_Data/y_val_200.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1NVfxcOI2uEJ","colab_type":"code","colab":{}},"source":["X_val=X_val.reshape(1562,200,200,3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"45q-It492zjs","colab_type":"code","outputId":"ad921ef7-cb54-4875-dfce-49e660050243","executionInfo":{"status":"ok","timestamp":1573141334350,"user_tz":0,"elapsed":24335,"user":{"displayName":"Nuno Carlos","photoUrl":"","userId":"16472806472979258588"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["## Load checkpoint and early stopping functions and other keras CNN modules\n","\n","from keras.callbacks import ReduceLROnPlateau , ModelCheckpoint, EarlyStopping\n","\n","lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=0.0001, patience=1, verbose=1)\n","filepath=\"/content/drive/My Drive/identify_pneumonia-master/model_2_weights/0.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n","earlystop=EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min', baseline=None, restore_best_weights=False)\n","\n","from keras.models import Sequential\n","from keras.layers import Dense , Activation\n","from keras.layers import Dropout\n","from keras.layers import Flatten\n","from keras.constraints import maxnorm\n","from keras.optimizers import SGD , RMSprop\n","from keras.layers import Conv2D , BatchNormalization\n","from keras.layers import MaxPooling2D\n","from keras.utils import np_utils\n","from keras import backend as K\n","\n","from sklearn.model_selection import GridSearchCV\n","from keras.wrappers.scikit_learn import KerasClassifier"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n","  warnings.warn('`epsilon` argument is deprecated and '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ZOLV1iNa23Ds","colab_type":"code","colab":{}},"source":["#Define the new loss function for counteract class imbalance\n","\n","from keras import backend as K\n","def weighted_categorical_crossentropy(weights):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","    \n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","    \n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","        loss = weighted_categorical_crossentropy(weights)\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","    \n","    weights = K.variable(weights)\n","        \n","    def loss(y_true, y_pred):\n","        # scale predictions so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","        # clip to prevent NaN's and Inf's\n","        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","        # calc\n","        loss = y_true * K.log(y_pred) * weights\n","        loss = -K.sum(loss, -1)\n","        return loss\n","    \n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4QnnGReB3GTB","colab_type":"code","colab":{}},"source":["from keras.optimizers import SGD , RMSprop , Adadelta , Adam\n","from keras.layers.normalization import BatchNormalization\n","from sklearn.metrics import recall_score\n","\n","#Build CNN model 2\n","\n","def create_model2(X_train, y_train, X_val, y_val, params):\n","\n","  model = Sequential()\n","  model.add(Conv2D(16, (3, 3), activation='relu', padding=\"same\", input_shape=(200,200,3), data_format='channels_last'))\n","  model.add(Conv2D(16, (3, 3), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  #model.add(BatchNormalization())\n","\n","  model.add(Conv2D(32, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(Conv2D(32, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Conv2D(64, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(Conv2D(64, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Conv2D(128, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(Conv2D(128, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Conv2D(256, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(Conv2D(256, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))    \n","\n","  model.add(Flatten())        \n","\n","##Define the hyperparameter configuration to Talos package\n","\n","  model.add(Dense(params['dense1'], activation='relu'))\n","  model.add(Dropout(params['drop1']))\n","  model.add(Dense(params['dense2'], activation='relu'))\n","  model.add(Dropout(params['drop2']))\n","  model.add(Dense(2 , activation='softmax'))\n","\n","  from sklearn import metrics\n","  from keras import backend as K\n","  from tensorflow.keras.metrics import Recall\n","  from keras import optimizers\n","\n","  ##Defined hyperparameters: loss function, otimizer, lr, decay, dense 1 units, dense 2 units, b size and dropout.\n","\n","  w=np.array([0.25,4])\n","  w_loss=weighted_categorical_crossentropy(w)\n","\n","  if params['otimizer']==adam:\n","      model.compile(loss=w_loss,\n","                      # here we add a regulizer normalization function from Talos\n","                      optimizer=params['otimizer'](lr=params['lr'], decay= params['decay'], beta_1=0.9, beta_2=0.999, amsgrad=False),\n","                      metrics=['acc'])\n","    \n","      print('yes')\n","  if params['otimizer']==sgd:\n","    model.compile(loss=w_loss,\n","                    # here we add a regulizer normalization function from Talos\n","                    optimizer=params['otimizer'](lr=params['lr'], decay= params['decay'], momentum=0.9, nesterov=True),\n","                    metrics=['acc'])\n","    print('yes2x')\n","\n","  history = model.fit(X_train, y_train,\n","                        batch_size=params['b_size'],\n","                        epochs=80,\n","                        validation_data=(X_val, y_val),\n","                        )\n","  tf.get_default_graph()\n","    # finally we have to make sure that history object and model are returned\n","  return history, model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SMXgrFW3Tkr","colab_type":"code","outputId":"c8e7461b-9edc-4933-da89-679555d4f204","executionInfo":{"status":"ok","timestamp":1572884318393,"user_tz":0,"elapsed":37152,"user":{"displayName":"Nuno Carlos","photoUrl":"","userId":"16472806472979258588"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install talos"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting talos\n","  Downloading https://files.pythonhosted.org/packages/1d/df/c352679af3259829dafa7d55f2d3e9fca201c848351cb3c841a062df001c/talos-0.6.3.tar.gz\n","Collecting wrangle\n","  Downloading https://files.pythonhosted.org/packages/85/35/bc729e377417613f2d062a890faea5d649ef1a554df21499e9c3a4a5501a/wrangle-0.6.7.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from talos) (1.17.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from talos) (0.25.2)\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from talos) (2.2.5)\n","Collecting astetik\n","  Downloading https://files.pythonhosted.org/packages/3c/ba/f8622951da73d9b47b45bb847112c388651f9c6e413e712954f260301d9f/astetik-1.9.9.tar.gz\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from talos) (0.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from talos) (4.28.1)\n","Collecting chances\n","  Downloading https://files.pythonhosted.org/packages/fa/d8/d61112d7476dc3074b855f1edd8556cde9b49b7106853f0b060109dd4c82/chances-0.1.9.tar.gz\n","Collecting kerasplotlib\n","  Downloading https://files.pythonhosted.org/packages/e8/2e/b8628bfef6a817da9be863f650cf67187676b10d27d94b23f248da35d2b4/kerasplotlib-0.1.4.tar.gz\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from talos) (2.21.0)\n","Collecting scipy==1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/e6/6d4edaceee6a110ecf6f318482f5229792f143e468b34a631f5a0899f56d/scipy-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (26.6MB)\n","\u001b[K     |████████████████████████████████| 26.6MB 1.9MB/s \n","\u001b[?25hRequirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from wrangle->talos) (0.10.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->talos) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->talos) (2.6.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->talos) (1.12.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->talos) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->talos) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->talos) (2.8.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->talos) (1.0.8)\n","Collecting geonamescache\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c1/efb823270c8526b2f4f3eb8c804c5a0a55277267ad2312f5eb47bd9cc370/geonamescache-1.1.0-py3-none-any.whl (830kB)\n","\u001b[K     |████████████████████████████████| 839kB 34.2MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->talos) (0.21.3)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (2019.9.11)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (3.0.4)\n","Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->wrangle->talos) (0.5.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->talos) (0.14.0)\n","Building wheels for collected packages: talos, wrangle, astetik, chances, kerasplotlib\n","  Building wheel for talos (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for talos: filename=talos-0.6.3-cp36-none-any.whl size=49626 sha256=92e6fa6f518569c351600612bf7ed05703d2a98e2ebad5b4b325470f59c10c5a\n","  Stored in directory: /root/.cache/pip/wheels/bb/d7/6b/86fd8b1fc7cfbd2c54796412f86efb5fb6a3a5c734014f6a66\n","  Building wheel for wrangle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrangle: filename=wrangle-0.6.7-cp36-none-any.whl size=49894 sha256=8b9e974d1d6dc753c18c97ef1caf7b4f95466ca71ac8360ad7bd36088dd1c699\n","  Stored in directory: /root/.cache/pip/wheels/bf/1b/50/d0403ce6ef269e364894da7b50db68db14c4ac62c577561e2d\n","  Building wheel for astetik (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for astetik: filename=astetik-1.9.9-cp36-none-any.whl size=56960 sha256=15724d4f1718716497cea6a8bb2c8c86584e744cb281941fe66c19e475577fb2\n","  Stored in directory: /root/.cache/pip/wheels/ae/70/21/c475cd079ec401dd6e1b9b1d42b4c38554ce12679bfb214aad\n","  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chances: filename=chances-0.1.9-cp36-none-any.whl size=41609 sha256=958685c8c4fcc24d9d9cf923ef57ebc1eb397ea05bf880d87afb8f9a9db94cef\n","  Stored in directory: /root/.cache/pip/wheels/75/33/46/c871b94249bd57d17797d049b3dff8e3a09c315afb67eb14c6\n","  Building wheel for kerasplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kerasplotlib: filename=kerasplotlib-0.1.4-cp36-none-any.whl size=3579 sha256=d1af312be28c9c7e0e7bd84ad03dd46d9aa435aca350c8eb2a7007c681c99fcb\n","  Stored in directory: /root/.cache/pip/wheels/36/6b/4c/e1fc6d7d8811940fbea1147b1519c7baa6933e4baeff904433\n","Successfully built talos wrangle astetik chances kerasplotlib\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: scipy, wrangle, geonamescache, astetik, chances, kerasplotlib, talos\n","  Found existing installation: scipy 1.3.1\n","    Uninstalling scipy-1.3.1:\n","      Successfully uninstalled scipy-1.3.1\n","Successfully installed astetik-1.9.9 chances-0.1.9 geonamescache-1.1.0 kerasplotlib-0.1.4 scipy-1.2.0 talos-0.6.3 wrangle-0.6.7\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["scipy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"gxJoqfR93UZm","colab_type":"code","colab":{}},"source":["import talos"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qqClgxYo3OXl","colab_type":"code","colab":{}},"source":["##Set the hyperparameter values\n","\n","from keras import optimizers\n","\n","sgd = optimizers.SGD\n","adam=optimizers.Adam\n","\n","p = {'drop1':[0.6], 'drop2':[0.5], 'lr':[0.001], 'decay':[1e-4], 'dense1':[256,512], 'dense2':[128,256],\n","     'otimizer':[adam], 'b_size':[64,128], 'w1':[[0.25,4]]}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PKaH_zekNrv_","colab_type":"code","outputId":"7458f63e-47ef-4134-e1ec-ff66a6ca15f0","executionInfo":{"status":"ok","timestamp":1572886762900,"user_tz":0,"elapsed":2393888,"user":{"displayName":"Nuno Carlos","photoUrl":"","userId":"16472806472979258588"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#start the scan by Talos (10% of total combinations: 6 experiments)\n","\n","t = talos.Scan(x=X_train,\n","            y=y_train,\n","            model=create_model2, \n","            params=p,\n","            x_val=X_val, y_val=y_val,\n","            experiment_name='hyperoptimization_weightedMODEL2',\n","            fraction_limit=0.1,\n","            random_method='quantum',\n","            print_params=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["{'b_size': 64, 'decay': 0.0001, 'dense1': 256, 'dense2': 128, 'drop1': 0.6, 'drop2': 0.5, 'epochs': 100, 'lr': 0.001, 'otimizer': <class 'keras.optimizers.Adam'>, 'w1': [0.25, 4]}\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","yes\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Train on 6500 samples, validate on 1562 samples\n","Epoch 1/80\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","6500/6500 [==============================] - 42s 6ms/step - loss: 0.4900 - acc: 0.1643 - val_loss: 0.4696 - val_acc: 0.1524\n","Epoch 2/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4815 - acc: 0.1569 - val_loss: 0.4690 - val_acc: 0.1524\n","Epoch 3/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4813 - acc: 0.1569 - val_loss: 0.4699 - val_acc: 0.1524\n","Epoch 4/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4749 - acc: 0.1569 - val_loss: 0.4601 - val_acc: 0.1524\n","Epoch 5/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4716 - acc: 0.1571 - val_loss: 0.4534 - val_acc: 0.1524\n","Epoch 6/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4629 - acc: 0.1569 - val_loss: 0.4633 - val_acc: 0.1524\n","Epoch 7/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4634 - acc: 0.1571 - val_loss: 0.4522 - val_acc: 0.1524\n","Epoch 8/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4607 - acc: 0.1583 - val_loss: 0.4504 - val_acc: 0.1524\n","Epoch 9/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4568 - acc: 0.1617 - val_loss: 0.4474 - val_acc: 0.1524\n","Epoch 10/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4532 - acc: 0.1571 - val_loss: 0.4505 - val_acc: 0.1524\n","Epoch 11/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4510 - acc: 0.1628 - val_loss: 0.4549 - val_acc: 0.1524\n","Epoch 12/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4505 - acc: 0.1597 - val_loss: 0.4436 - val_acc: 0.1524\n","Epoch 13/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4448 - acc: 0.1620 - val_loss: 0.4445 - val_acc: 0.1524\n","Epoch 14/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4407 - acc: 0.1823 - val_loss: 0.4119 - val_acc: 0.1895\n","Epoch 15/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4143 - acc: 0.2969 - val_loss: 0.4108 - val_acc: 0.1850\n","Epoch 16/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3902 - acc: 0.3725 - val_loss: 0.3859 - val_acc: 0.6024\n","Epoch 17/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3742 - acc: 0.4695 - val_loss: 0.4055 - val_acc: 0.1786\n","Epoch 18/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3622 - acc: 0.4498 - val_loss: 0.3983 - val_acc: 0.7100\n","Epoch 19/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3507 - acc: 0.5162 - val_loss: 0.3586 - val_acc: 0.4001\n","Epoch 20/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3350 - acc: 0.5511 - val_loss: 0.3706 - val_acc: 0.2836\n","Epoch 21/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3244 - acc: 0.5678 - val_loss: 0.3547 - val_acc: 0.7228\n","Epoch 22/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3139 - acc: 0.6074 - val_loss: 0.3292 - val_acc: 0.6236\n","Epoch 23/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3032 - acc: 0.6234 - val_loss: 0.3504 - val_acc: 0.4526\n","Epoch 24/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2979 - acc: 0.6315 - val_loss: 0.3150 - val_acc: 0.7497\n","Epoch 25/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2787 - acc: 0.6817 - val_loss: 0.3150 - val_acc: 0.6024\n","Epoch 26/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2800 - acc: 0.6645 - val_loss: 0.3303 - val_acc: 0.5922\n","Epoch 27/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2716 - acc: 0.6791 - val_loss: 0.3542 - val_acc: 0.7561\n","Epoch 28/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2456 - acc: 0.7175 - val_loss: 0.3351 - val_acc: 0.7529\n","Epoch 29/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2406 - acc: 0.7138 - val_loss: 0.3135 - val_acc: 0.6684\n","Epoch 30/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2305 - acc: 0.7260 - val_loss: 0.3273 - val_acc: 0.6018\n","Epoch 31/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2210 - acc: 0.7494 - val_loss: 0.3339 - val_acc: 0.6620\n","Epoch 32/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2153 - acc: 0.7458 - val_loss: 0.3615 - val_acc: 0.7535\n","Epoch 33/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2003 - acc: 0.7712 - val_loss: 0.3419 - val_acc: 0.6639\n","Epoch 34/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1915 - acc: 0.7760 - val_loss: 0.4972 - val_acc: 0.7855\n","Epoch 35/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2149 - acc: 0.7575 - val_loss: 0.3884 - val_acc: 0.7266\n","Epoch 36/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1808 - acc: 0.7960 - val_loss: 0.4032 - val_acc: 0.7401\n","Epoch 37/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1730 - acc: 0.8026 - val_loss: 0.5532 - val_acc: 0.7900\n","Epoch 38/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1587 - acc: 0.8288 - val_loss: 0.5277 - val_acc: 0.7990\n","Epoch 39/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1515 - acc: 0.8303 - val_loss: 0.4410 - val_acc: 0.7612\n","Epoch 40/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1556 - acc: 0.8298 - val_loss: 0.5230 - val_acc: 0.7964\n","Epoch 41/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1534 - acc: 0.8340 - val_loss: 0.5403 - val_acc: 0.8348\n","Epoch 42/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1312 - acc: 0.8545 - val_loss: 0.6244 - val_acc: 0.8246\n","Epoch 43/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1248 - acc: 0.8595 - val_loss: 0.7395 - val_acc: 0.8380\n","Epoch 44/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1088 - acc: 0.8883 - val_loss: 0.8674 - val_acc: 0.8502\n","Epoch 45/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1208 - acc: 0.8709 - val_loss: 0.6260 - val_acc: 0.8297\n","Epoch 46/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1162 - acc: 0.8757 - val_loss: 0.6487 - val_acc: 0.8329\n","Epoch 47/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1251 - acc: 0.8649 - val_loss: 0.7716 - val_acc: 0.8271\n","Epoch 48/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1153 - acc: 0.8800 - val_loss: 0.6527 - val_acc: 0.8342\n","Epoch 49/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0961 - acc: 0.8975 - val_loss: 0.9357 - val_acc: 0.8656\n","Epoch 50/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1172 - acc: 0.8734 - val_loss: 0.6410 - val_acc: 0.8169\n","Epoch 51/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0885 - acc: 0.9120 - val_loss: 0.6850 - val_acc: 0.7804\n","Epoch 52/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1244 - acc: 0.8662 - val_loss: 0.4912 - val_acc: 0.7593\n","Epoch 53/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0940 - acc: 0.8969 - val_loss: 0.9673 - val_acc: 0.8604\n","Epoch 54/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0936 - acc: 0.9028 - val_loss: 0.9560 - val_acc: 0.8515\n","Epoch 55/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0693 - acc: 0.9302 - val_loss: 1.1151 - val_acc: 0.8643\n","Epoch 56/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0693 - acc: 0.9329 - val_loss: 1.0351 - val_acc: 0.8726\n","Epoch 57/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0796 - acc: 0.9242 - val_loss: 0.7274 - val_acc: 0.7817\n","Epoch 58/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0923 - acc: 0.9074 - val_loss: 0.8282 - val_acc: 0.8265\n","Epoch 59/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0738 - acc: 0.9252 - val_loss: 1.5845 - val_acc: 0.8880\n","Epoch 60/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0704 - acc: 0.9383 - val_loss: 1.2403 - val_acc: 0.8508\n","Epoch 61/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0625 - acc: 0.9352 - val_loss: 1.2459 - val_acc: 0.8681\n","Epoch 62/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0591 - acc: 0.9429 - val_loss: 1.2208 - val_acc: 0.8361\n","Epoch 63/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0837 - acc: 0.9195 - val_loss: 0.8716 - val_acc: 0.8156\n","Epoch 64/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0732 - acc: 0.9252 - val_loss: 1.0988 - val_acc: 0.8553\n","Epoch 65/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0735 - acc: 0.9308 - val_loss: 1.0204 - val_acc: 0.8496\n","Epoch 66/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0785 - acc: 0.9286 - val_loss: 1.0134 - val_acc: 0.8483\n","Epoch 67/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0722 - acc: 0.9394 - val_loss: 0.8216 - val_acc: 0.8073\n","Epoch 68/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0788 - acc: 0.9285 - val_loss: 1.4025 - val_acc: 0.8707\n","Epoch 69/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0584 - acc: 0.9448 - val_loss: 1.0065 - val_acc: 0.8297\n","Epoch 70/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0556 - acc: 0.9469 - val_loss: 1.0448 - val_acc: 0.8342\n","Epoch 71/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0458 - acc: 0.9549 - val_loss: 1.3220 - val_acc: 0.8585\n","Epoch 72/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0740 - acc: 0.9303 - val_loss: 1.3931 - val_acc: 0.8822\n","Epoch 73/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0504 - acc: 0.9571 - val_loss: 1.5305 - val_acc: 0.8905\n","Epoch 74/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0529 - acc: 0.9460 - val_loss: 1.4611 - val_acc: 0.8790\n","Epoch 75/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0444 - acc: 0.9608 - val_loss: 1.2955 - val_acc: 0.8694\n","Epoch 76/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0518 - acc: 0.9532 - val_loss: 1.4381 - val_acc: 0.8912\n","Epoch 77/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0404 - acc: 0.9575 - val_loss: 1.8044 - val_acc: 0.8828\n","Epoch 78/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0486 - acc: 0.9614 - val_loss: 1.0914 - val_acc: 0.8374\n","Epoch 79/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0526 - acc: 0.9506 - val_loss: 1.3571 - val_acc: 0.8739\n","Epoch 80/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0439 - acc: 0.9628 - val_loss: 1.3377 - val_acc: 0.8720\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r100%|██████████| 1/1 [39:51<00:00, 2391.11s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Mcw99nEL3abr","colab_type":"code","outputId":"4ca03f06-bc35-4867-a7d2-a6d9880f3ee4","executionInfo":{"status":"ok","timestamp":1572747669365,"user_tz":0,"elapsed":14308052,"user":{"displayName":"Nuno Carlos","photoUrl":"","userId":"16472806472979258588"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["t = talos.Scan(x=X_train,\n","            y=y_train,\n","            model=create_model3, \n","            params=p,\n","            x_val=X_val, y_val=y_val,\n","            experiment_name='hyperoptimization_weightedMODEL2',\n","            fraction_limit=0.05,\n","            random_method='quantum',\n","            print_params=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","\n","\n","  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'b_size': 64, 'decay': 0.0001, 'dense1': 256, 'dense2': 128, 'drop1': 0.6, 'drop2': 0.5, 'epochs': 75, 'lr': 0.01, 'otimizer': <class 'keras.optimizers.SGD'>, 'w1': [0.25, 4]}\n","WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n","yes2x\n","Train on 6500 samples, validate on 1562 samples\n","Epoch 1/80\n","6500/6500 [==============================] - 32s 5ms/step - loss: 0.4862 - acc: 0.1657 - val_loss: 0.4689 - val_acc: 0.1524\n","Epoch 2/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4739 - acc: 0.1569 - val_loss: 0.4689 - val_acc: 0.1524\n","Epoch 3/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4744 - acc: 0.1569 - val_loss: 0.4694 - val_acc: 0.1524\n","Epoch 4/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4738 - acc: 0.1569 - val_loss: 0.4688 - val_acc: 0.1524\n","Epoch 5/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4735 - acc: 0.1569 - val_loss: 0.4688 - val_acc: 0.1524\n","Epoch 6/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4742 - acc: 0.1569 - val_loss: 0.4689 - val_acc: 0.1524\n","Epoch 7/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4724 - acc: 0.1569 - val_loss: 0.4675 - val_acc: 0.1524\n","Epoch 8/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4701 - acc: 0.1569 - val_loss: 0.4633 - val_acc: 0.1524\n","Epoch 9/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4604 - acc: 0.1569 - val_loss: 0.4241 - val_acc: 0.1524\n","Epoch 10/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4328 - acc: 0.2266 - val_loss: 0.3919 - val_acc: 0.1837\n","Epoch 11/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4100 - acc: 0.3165 - val_loss: 0.3804 - val_acc: 0.2554\n","Epoch 12/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3966 - acc: 0.3818 - val_loss: 0.3617 - val_acc: 0.4373\n","Epoch 13/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3943 - acc: 0.3886 - val_loss: 0.3666 - val_acc: 0.3028\n","Epoch 14/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3844 - acc: 0.4165 - val_loss: 0.3490 - val_acc: 0.4840\n","Epoch 15/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3783 - acc: 0.4315 - val_loss: 0.3410 - val_acc: 0.5416\n","Epoch 16/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3749 - acc: 0.4691 - val_loss: 0.3581 - val_acc: 0.3175\n","Epoch 17/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3744 - acc: 0.4255 - val_loss: 0.3403 - val_acc: 0.5160\n","Epoch 18/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3675 - acc: 0.4749 - val_loss: 0.3363 - val_acc: 0.5826\n","Epoch 19/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3550 - acc: 0.5063 - val_loss: 0.3428 - val_acc: 0.5781\n","Epoch 20/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3575 - acc: 0.4925 - val_loss: 0.3514 - val_acc: 0.4046\n","Epoch 21/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3520 - acc: 0.5063 - val_loss: 0.3274 - val_acc: 0.4603\n","Epoch 22/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3458 - acc: 0.5054 - val_loss: 0.3703 - val_acc: 0.2945\n","Epoch 23/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.3376 - acc: 0.5380 - val_loss: 0.3286 - val_acc: 0.4917\n","Epoch 24/80\n","6500/6500 [==============================] - 32s 5ms/step - loss: 0.3335 - acc: 0.5457 - val_loss: 0.3114 - val_acc: 0.6364\n","Epoch 25/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.3346 - acc: 0.5518 - val_loss: 0.3255 - val_acc: 0.5512\n","Epoch 26/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.3158 - acc: 0.5875 - val_loss: 0.3048 - val_acc: 0.5992\n","Epoch 27/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.3078 - acc: 0.6062 - val_loss: 0.3025 - val_acc: 0.5954\n","Epoch 28/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.3006 - acc: 0.6254 - val_loss: 0.3260 - val_acc: 0.7817\n","Epoch 29/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2985 - acc: 0.6423 - val_loss: 0.2958 - val_acc: 0.5877\n","Epoch 30/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2918 - acc: 0.6503 - val_loss: 0.2927 - val_acc: 0.5858\n","Epoch 31/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2768 - acc: 0.6772 - val_loss: 0.2920 - val_acc: 0.6146\n","Epoch 32/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2781 - acc: 0.6909 - val_loss: 0.2853 - val_acc: 0.6889\n","Epoch 33/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2811 - acc: 0.6635 - val_loss: 0.2851 - val_acc: 0.6716\n","Epoch 34/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2620 - acc: 0.7060 - val_loss: 0.3015 - val_acc: 0.7196\n","Epoch 35/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2518 - acc: 0.7252 - val_loss: 0.2981 - val_acc: 0.5685\n","Epoch 36/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2516 - acc: 0.7125 - val_loss: 0.2888 - val_acc: 0.7612\n","Epoch 37/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2430 - acc: 0.7286 - val_loss: 0.2982 - val_acc: 0.7561\n","Epoch 38/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2346 - acc: 0.7380 - val_loss: 0.3079 - val_acc: 0.7810\n","Epoch 39/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2243 - acc: 0.7549 - val_loss: 0.3784 - val_acc: 0.6434\n","Epoch 40/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2365 - acc: 0.7352 - val_loss: 0.3157 - val_acc: 0.7382\n","Epoch 41/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2170 - acc: 0.7646 - val_loss: 0.3239 - val_acc: 0.7586\n","Epoch 42/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2063 - acc: 0.7818 - val_loss: 0.3678 - val_acc: 0.8086\n","Epoch 43/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.2009 - acc: 0.7849 - val_loss: 0.3023 - val_acc: 0.8041\n","Epoch 44/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.1921 - acc: 0.7971 - val_loss: 0.4799 - val_acc: 0.8809\n","Epoch 45/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.1878 - acc: 0.7923 - val_loss: 0.3973 - val_acc: 0.8143\n","Epoch 46/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.1804 - acc: 0.8005 - val_loss: 0.3331 - val_acc: 0.7996\n","Epoch 47/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.1740 - acc: 0.8168 - val_loss: 0.4193 - val_acc: 0.8572\n","Epoch 48/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1685 - acc: 0.8222 - val_loss: 0.4517 - val_acc: 0.8675\n","Epoch 49/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1652 - acc: 0.8198 - val_loss: 0.4112 - val_acc: 0.8342\n","Epoch 50/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1653 - acc: 0.8226 - val_loss: 0.5623 - val_acc: 0.8694\n","Epoch 51/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1635 - acc: 0.8422 - val_loss: 0.4801 - val_acc: 0.8668\n","Epoch 52/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1581 - acc: 0.8374 - val_loss: 0.3952 - val_acc: 0.8239\n","Epoch 53/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1427 - acc: 0.8583 - val_loss: 0.4122 - val_acc: 0.8156\n","Epoch 54/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1257 - acc: 0.8705 - val_loss: 0.4674 - val_acc: 0.8585\n","Epoch 55/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1257 - acc: 0.8808 - val_loss: 0.4508 - val_acc: 0.8406\n","Epoch 56/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1181 - acc: 0.8735 - val_loss: 0.4504 - val_acc: 0.7894\n","Epoch 57/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1180 - acc: 0.8851 - val_loss: 0.4564 - val_acc: 0.7138\n","Epoch 58/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1203 - acc: 0.8875 - val_loss: 0.4789 - val_acc: 0.7855\n","Epoch 59/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0961 - acc: 0.9043 - val_loss: 0.7267 - val_acc: 0.8841\n","Epoch 60/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1195 - acc: 0.8828 - val_loss: 0.7560 - val_acc: 0.8899\n","Epoch 61/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1250 - acc: 0.8782 - val_loss: 0.3572 - val_acc: 0.7599\n","Epoch 62/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0986 - acc: 0.9066 - val_loss: 0.6927 - val_acc: 0.8752\n","Epoch 63/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0823 - acc: 0.9185 - val_loss: 0.8361 - val_acc: 0.8816\n","Epoch 64/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0967 - acc: 0.9088 - val_loss: 0.5697 - val_acc: 0.8604\n","Epoch 65/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0888 - acc: 0.9177 - val_loss: 0.6795 - val_acc: 0.8803\n","Epoch 66/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0900 - acc: 0.9129 - val_loss: 0.9335 - val_acc: 0.9065\n","Epoch 67/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0723 - acc: 0.9297 - val_loss: 0.5633 - val_acc: 0.8323\n","Epoch 68/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1020 - acc: 0.9025 - val_loss: 0.7583 - val_acc: 0.8816\n","Epoch 69/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0756 - acc: 0.9308 - val_loss: 0.6041 - val_acc: 0.8355\n","Epoch 70/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0748 - acc: 0.9343 - val_loss: 0.7393 - val_acc: 0.8598\n","Epoch 71/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0542 - acc: 0.9482 - val_loss: 0.9258 - val_acc: 0.8905\n","Epoch 72/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0789 - acc: 0.9298 - val_loss: 0.6127 - val_acc: 0.8624\n","Epoch 73/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0642 - acc: 0.9422 - val_loss: 1.1453 - val_acc: 0.8944\n","Epoch 74/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0593 - acc: 0.9418 - val_loss: 0.9308 - val_acc: 0.8790\n","Epoch 75/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0551 - acc: 0.9485 - val_loss: 0.9720 - val_acc: 0.8803\n","Epoch 76/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0423 - acc: 0.9620 - val_loss: 1.2105 - val_acc: 0.9059\n","Epoch 77/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0549 - acc: 0.9534 - val_loss: 1.0495 - val_acc: 0.8828\n","Epoch 78/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0755 - acc: 0.9378 - val_loss: 0.8008 - val_acc: 0.8630\n","Epoch 79/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0503 - acc: 0.9540 - val_loss: 0.7734 - val_acc: 0.8560\n","Epoch 80/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0529 - acc: 0.9571 - val_loss: 0.9875 - val_acc: 0.8809\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n"," 17%|█▋        | 1/6 [40:42<3:23:32, 2442.54s/it]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'b_size': 128, 'decay': 1e-06, 'dense1': 512, 'dense2': 256, 'drop1': 0.6, 'drop2': 0.5, 'epochs': 100, 'lr': 0.01, 'otimizer': <class 'keras.optimizers.SGD'>, 'w1': [0.25, 4]}\n","WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n","yes2x\n","Train on 6500 samples, validate on 1562 samples\n","Epoch 1/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4919 - acc: 0.1692 - val_loss: 0.4687 - val_acc: 0.1524\n","Epoch 2/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4749 - acc: 0.1569 - val_loss: 0.4686 - val_acc: 0.1524\n","Epoch 3/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4739 - acc: 0.1569 - val_loss: 0.4688 - val_acc: 0.1524\n","Epoch 4/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4723 - acc: 0.1569 - val_loss: 0.4672 - val_acc: 0.1524\n","Epoch 5/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4729 - acc: 0.1569 - val_loss: 0.4655 - val_acc: 0.1524\n","Epoch 6/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4691 - acc: 0.1569 - val_loss: 0.4589 - val_acc: 0.1524\n","Epoch 7/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4608 - acc: 0.1569 - val_loss: 0.4392 - val_acc: 0.1524\n","Epoch 8/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4301 - acc: 0.2217 - val_loss: 0.3937 - val_acc: 0.1991\n","Epoch 9/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4106 - acc: 0.3123 - val_loss: 0.3756 - val_acc: 0.2932\n","Epoch 10/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3981 - acc: 0.3826 - val_loss: 0.3948 - val_acc: 0.2228\n","Epoch 11/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3920 - acc: 0.3871 - val_loss: 0.3643 - val_acc: 0.4648\n","Epoch 12/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3803 - acc: 0.4525 - val_loss: 0.3658 - val_acc: 0.4193\n","Epoch 13/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3762 - acc: 0.4351 - val_loss: 0.3702 - val_acc: 0.2631\n","Epoch 14/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3757 - acc: 0.4357 - val_loss: 0.3592 - val_acc: 0.3848\n","Epoch 15/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3732 - acc: 0.4526 - val_loss: 0.3376 - val_acc: 0.5723\n","Epoch 16/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3623 - acc: 0.4928 - val_loss: 0.3439 - val_acc: 0.4110\n","Epoch 17/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3597 - acc: 0.4805 - val_loss: 0.3445 - val_acc: 0.7202\n","Epoch 18/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3552 - acc: 0.5155 - val_loss: 0.3324 - val_acc: 0.5743\n","Epoch 19/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3552 - acc: 0.5062 - val_loss: 0.3421 - val_acc: 0.3944\n","Epoch 20/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3427 - acc: 0.5335 - val_loss: 0.3216 - val_acc: 0.6242\n","Epoch 21/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3459 - acc: 0.5435 - val_loss: 0.3207 - val_acc: 0.5352\n","Epoch 22/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3302 - acc: 0.5675 - val_loss: 0.3255 - val_acc: 0.4853\n","Epoch 23/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3296 - acc: 0.5675 - val_loss: 0.3205 - val_acc: 0.6229\n","Epoch 24/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3225 - acc: 0.5877 - val_loss: 0.3333 - val_acc: 0.3617\n","Epoch 25/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3251 - acc: 0.5688 - val_loss: 0.3051 - val_acc: 0.6056\n","Epoch 26/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3090 - acc: 0.6220 - val_loss: 0.3399 - val_acc: 0.4609\n","Epoch 27/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3097 - acc: 0.6168 - val_loss: 0.3090 - val_acc: 0.5666\n","Epoch 28/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3067 - acc: 0.6232 - val_loss: 0.3011 - val_acc: 0.6076\n","Epoch 29/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2992 - acc: 0.6383 - val_loss: 0.3087 - val_acc: 0.5864\n","Epoch 30/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2919 - acc: 0.6466 - val_loss: 0.2987 - val_acc: 0.7093\n","Epoch 31/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2837 - acc: 0.6611 - val_loss: 0.3230 - val_acc: 0.8092\n","Epoch 32/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2753 - acc: 0.6791 - val_loss: 0.3079 - val_acc: 0.6857\n","Epoch 33/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2691 - acc: 0.6942 - val_loss: 0.2868 - val_acc: 0.6773\n","Epoch 34/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2620 - acc: 0.7046 - val_loss: 0.2970 - val_acc: 0.6837\n","Epoch 35/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2603 - acc: 0.7012 - val_loss: 0.3040 - val_acc: 0.6882\n","Epoch 36/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2566 - acc: 0.7045 - val_loss: 0.3078 - val_acc: 0.7631\n","Epoch 37/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2384 - acc: 0.7317 - val_loss: 0.3638 - val_acc: 0.8175\n","Epoch 38/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2358 - acc: 0.7457 - val_loss: 0.3380 - val_acc: 0.7875\n","Epoch 39/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2268 - acc: 0.7505 - val_loss: 0.2882 - val_acc: 0.6581\n","Epoch 40/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2318 - acc: 0.7435 - val_loss: 0.2996 - val_acc: 0.7113\n","Epoch 41/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2201 - acc: 0.7525 - val_loss: 0.2954 - val_acc: 0.6953\n","Epoch 42/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2184 - acc: 0.7577 - val_loss: 0.2936 - val_acc: 0.7586\n","Epoch 43/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2074 - acc: 0.7700 - val_loss: 0.3503 - val_acc: 0.8367\n","Epoch 44/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1912 - acc: 0.7998 - val_loss: 0.3280 - val_acc: 0.7721\n","Epoch 45/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1905 - acc: 0.8017 - val_loss: 0.3209 - val_acc: 0.7977\n","Epoch 46/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1818 - acc: 0.7966 - val_loss: 0.3749 - val_acc: 0.8047\n","Epoch 47/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1807 - acc: 0.8071 - val_loss: 0.3638 - val_acc: 0.8054\n","Epoch 48/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1613 - acc: 0.8222 - val_loss: 0.4078 - val_acc: 0.8316\n","Epoch 49/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1709 - acc: 0.8160 - val_loss: 0.3694 - val_acc: 0.8047\n","Epoch 50/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1620 - acc: 0.8317 - val_loss: 0.3503 - val_acc: 0.7522\n","Epoch 51/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1563 - acc: 0.8312 - val_loss: 0.3724 - val_acc: 0.8163\n","Epoch 52/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1449 - acc: 0.8431 - val_loss: 0.3820 - val_acc: 0.7855\n","Epoch 53/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1407 - acc: 0.8555 - val_loss: 0.3570 - val_acc: 0.7465\n","Epoch 54/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1299 - acc: 0.8608 - val_loss: 0.4234 - val_acc: 0.8239\n","Epoch 55/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1463 - acc: 0.8478 - val_loss: 0.5310 - val_acc: 0.8681\n","Epoch 56/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1253 - acc: 0.8632 - val_loss: 0.4711 - val_acc: 0.8079\n","Epoch 57/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1232 - acc: 0.8751 - val_loss: 0.5167 - val_acc: 0.7644\n","Epoch 58/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1132 - acc: 0.8834 - val_loss: 0.5140 - val_acc: 0.8303\n","Epoch 59/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1250 - acc: 0.8743 - val_loss: 0.5032 - val_acc: 0.8585\n","Epoch 60/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1151 - acc: 0.8862 - val_loss: 0.4125 - val_acc: 0.7612\n","Epoch 61/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1061 - acc: 0.8925 - val_loss: 0.4788 - val_acc: 0.8009\n","Epoch 62/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1035 - acc: 0.8982 - val_loss: 0.5536 - val_acc: 0.8656\n","Epoch 63/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1198 - acc: 0.8840 - val_loss: 0.5905 - val_acc: 0.8905\n","Epoch 64/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0807 - acc: 0.9217 - val_loss: 0.5588 - val_acc: 0.8476\n","Epoch 65/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0833 - acc: 0.9185 - val_loss: 0.7814 - val_acc: 0.8963\n","Epoch 66/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1078 - acc: 0.8966 - val_loss: 0.6635 - val_acc: 0.8649\n","Epoch 67/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0867 - acc: 0.9151 - val_loss: 0.8620 - val_acc: 0.9078\n","Epoch 68/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0923 - acc: 0.9138 - val_loss: 0.6360 - val_acc: 0.8771\n","Epoch 69/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0653 - acc: 0.9374 - val_loss: 0.8665 - val_acc: 0.8784\n","Epoch 70/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0886 - acc: 0.9123 - val_loss: 0.6236 - val_acc: 0.8643\n","Epoch 71/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0737 - acc: 0.9260 - val_loss: 0.8185 - val_acc: 0.8713\n","Epoch 72/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0594 - acc: 0.9426 - val_loss: 0.7774 - val_acc: 0.8777\n","Epoch 73/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0626 - acc: 0.9418 - val_loss: 0.7629 - val_acc: 0.8694\n","Epoch 74/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0516 - acc: 0.9498 - val_loss: 0.9850 - val_acc: 0.8956\n","Epoch 75/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0673 - acc: 0.9394 - val_loss: 0.7037 - val_acc: 0.8367\n","Epoch 76/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0562 - acc: 0.9462 - val_loss: 0.8011 - val_acc: 0.8444\n","Epoch 77/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0569 - acc: 0.9455 - val_loss: 0.7472 - val_acc: 0.8502\n","Epoch 78/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0534 - acc: 0.9528 - val_loss: 0.6284 - val_acc: 0.8688\n","Epoch 79/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0647 - acc: 0.9435 - val_loss: 0.6685 - val_acc: 0.8444\n","Epoch 80/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0375 - acc: 0.9669 - val_loss: 0.9477 - val_acc: 0.8924\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n"," 33%|███▎      | 2/6 [1:19:46<2:40:51, 2413.00s/it]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'b_size': 128, 'decay': 0.0001, 'dense1': 512, 'dense2': 256, 'drop1': 0.6, 'drop2': 0.5, 'epochs': 75, 'lr': 0.001, 'otimizer': <class 'keras.optimizers.Adam'>, 'w1': [0.25, 4]}\n","yes\n","Train on 6500 samples, validate on 1562 samples\n","Epoch 1/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4922 - acc: 0.1623 - val_loss: 0.4732 - val_acc: 0.1524\n","Epoch 2/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4801 - acc: 0.1569 - val_loss: 0.4632 - val_acc: 0.1524\n","Epoch 3/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.4507 - acc: 0.1895 - val_loss: 0.3889 - val_acc: 0.2420\n","Epoch 4/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4121 - acc: 0.3217 - val_loss: 0.3856 - val_acc: 0.3073\n","Epoch 5/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3960 - acc: 0.3975 - val_loss: 0.3729 - val_acc: 0.3387\n","Epoch 6/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3927 - acc: 0.4091 - val_loss: 0.3792 - val_acc: 0.3476\n","Epoch 7/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3763 - acc: 0.4455 - val_loss: 0.3472 - val_acc: 0.5403\n","Epoch 8/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3645 - acc: 0.4697 - val_loss: 0.3452 - val_acc: 0.4462\n","Epoch 9/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3530 - acc: 0.5188 - val_loss: 0.3391 - val_acc: 0.4872\n","Epoch 10/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3474 - acc: 0.5155 - val_loss: 0.3398 - val_acc: 0.5973\n","Epoch 11/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3442 - acc: 0.5358 - val_loss: 0.3237 - val_acc: 0.7036\n","Epoch 12/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3407 - acc: 0.5452 - val_loss: 0.3302 - val_acc: 0.6088\n","Epoch 13/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3199 - acc: 0.5794 - val_loss: 0.3386 - val_acc: 0.4366\n","Epoch 14/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3224 - acc: 0.5840 - val_loss: 0.3265 - val_acc: 0.4603\n","Epoch 15/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3216 - acc: 0.5758 - val_loss: 0.3072 - val_acc: 0.5903\n","Epoch 16/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3032 - acc: 0.6185 - val_loss: 0.3013 - val_acc: 0.6396\n","Epoch 17/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3019 - acc: 0.6245 - val_loss: 0.3064 - val_acc: 0.6517\n","Epoch 18/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2940 - acc: 0.6411 - val_loss: 0.2993 - val_acc: 0.6882\n","Epoch 19/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2817 - acc: 0.6628 - val_loss: 0.2956 - val_acc: 0.6837\n","Epoch 20/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2804 - acc: 0.6835 - val_loss: 0.3141 - val_acc: 0.7561\n","Epoch 21/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2592 - acc: 0.7063 - val_loss: 0.2878 - val_acc: 0.6837\n","Epoch 22/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2508 - acc: 0.7206 - val_loss: 0.3188 - val_acc: 0.7702\n","Epoch 23/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2360 - acc: 0.7445 - val_loss: 0.3017 - val_acc: 0.6569\n","Epoch 24/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2431 - acc: 0.7338 - val_loss: 0.3003 - val_acc: 0.6216\n","Epoch 25/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2435 - acc: 0.7331 - val_loss: 0.3457 - val_acc: 0.7618\n","Epoch 26/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2518 - acc: 0.7275 - val_loss: 0.2999 - val_acc: 0.5755\n","Epoch 27/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2283 - acc: 0.7375 - val_loss: 0.3648 - val_acc: 0.7996\n","Epoch 28/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2301 - acc: 0.7592 - val_loss: 0.3529 - val_acc: 0.7932\n","Epoch 29/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2128 - acc: 0.7783 - val_loss: 0.3134 - val_acc: 0.7209\n","Epoch 30/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1990 - acc: 0.7918 - val_loss: 0.3731 - val_acc: 0.7759\n","Epoch 31/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1828 - acc: 0.8188 - val_loss: 0.3703 - val_acc: 0.6927\n","Epoch 32/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1867 - acc: 0.8014 - val_loss: 0.4196 - val_acc: 0.7714\n","Epoch 33/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1769 - acc: 0.8111 - val_loss: 0.4298 - val_acc: 0.7772\n","Epoch 34/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1876 - acc: 0.8040 - val_loss: 0.3714 - val_acc: 0.7305\n","Epoch 35/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1680 - acc: 0.8322 - val_loss: 0.3770 - val_acc: 0.7663\n","Epoch 36/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1683 - acc: 0.8140 - val_loss: 0.3335 - val_acc: 0.7650\n","Epoch 37/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1718 - acc: 0.8266 - val_loss: 0.4383 - val_acc: 0.8361\n","Epoch 38/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1619 - acc: 0.8452 - val_loss: 0.4079 - val_acc: 0.7977\n","Epoch 39/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1473 - acc: 0.8565 - val_loss: 0.5348 - val_acc: 0.8438\n","Epoch 40/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1596 - acc: 0.8397 - val_loss: 0.4735 - val_acc: 0.8252\n","Epoch 41/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1370 - acc: 0.8683 - val_loss: 0.5673 - val_acc: 0.8438\n","Epoch 42/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1220 - acc: 0.8928 - val_loss: 0.4636 - val_acc: 0.7862\n","Epoch 43/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1399 - acc: 0.8560 - val_loss: 0.4983 - val_acc: 0.8636\n","Epoch 44/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1270 - acc: 0.8777 - val_loss: 0.5989 - val_acc: 0.8579\n","Epoch 45/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1103 - acc: 0.8940 - val_loss: 0.7778 - val_acc: 0.8572\n","Epoch 46/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1032 - acc: 0.9002 - val_loss: 0.8411 - val_acc: 0.8412\n","Epoch 47/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1044 - acc: 0.9023 - val_loss: 0.5374 - val_acc: 0.7971\n","Epoch 48/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1047 - acc: 0.8925 - val_loss: 0.8899 - val_acc: 0.8566\n","Epoch 49/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0873 - acc: 0.9162 - val_loss: 0.7249 - val_acc: 0.8335\n","Epoch 50/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1198 - acc: 0.8654 - val_loss: 0.7912 - val_acc: 0.8476\n","Epoch 51/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1172 - acc: 0.8855 - val_loss: 0.6415 - val_acc: 0.8399\n","Epoch 52/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1014 - acc: 0.8952 - val_loss: 0.4767 - val_acc: 0.7561\n","Epoch 53/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1146 - acc: 0.8857 - val_loss: 0.8272 - val_acc: 0.8572\n","Epoch 54/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0733 - acc: 0.9349 - val_loss: 0.8724 - val_acc: 0.8656\n","Epoch 55/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0813 - acc: 0.9220 - val_loss: 0.7415 - val_acc: 0.8143\n","Epoch 56/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1035 - acc: 0.9085 - val_loss: 0.8063 - val_acc: 0.8348\n","Epoch 57/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0817 - acc: 0.9186 - val_loss: 0.7905 - val_acc: 0.8406\n","Epoch 58/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0690 - acc: 0.9362 - val_loss: 0.9645 - val_acc: 0.8553\n","Epoch 59/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0695 - acc: 0.9326 - val_loss: 0.8203 - val_acc: 0.8239\n","Epoch 60/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0857 - acc: 0.9240 - val_loss: 0.8397 - val_acc: 0.8496\n","Epoch 61/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0887 - acc: 0.9151 - val_loss: 0.8061 - val_acc: 0.8438\n","Epoch 62/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0605 - acc: 0.9414 - val_loss: 0.8606 - val_acc: 0.8560\n","Epoch 63/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0748 - acc: 0.9317 - val_loss: 0.7457 - val_acc: 0.8451\n","Epoch 64/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0502 - acc: 0.9502 - val_loss: 1.3814 - val_acc: 0.8873\n","Epoch 65/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0457 - acc: 0.9597 - val_loss: 1.1246 - val_acc: 0.8700\n","Epoch 66/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0444 - acc: 0.9609 - val_loss: 1.0428 - val_acc: 0.8566\n","Epoch 67/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0517 - acc: 0.9489 - val_loss: 0.9814 - val_acc: 0.8752\n","Epoch 68/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0535 - acc: 0.9457 - val_loss: 1.0882 - val_acc: 0.8675\n","Epoch 69/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0443 - acc: 0.9566 - val_loss: 1.2815 - val_acc: 0.8720\n","Epoch 70/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0361 - acc: 0.9660 - val_loss: 1.3520 - val_acc: 0.8572\n","Epoch 71/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0900 - acc: 0.9285 - val_loss: 0.6400 - val_acc: 0.8233\n","Epoch 72/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0731 - acc: 0.9298 - val_loss: 0.9062 - val_acc: 0.8604\n","Epoch 73/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0462 - acc: 0.9574 - val_loss: 1.5182 - val_acc: 0.8809\n","Epoch 74/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0342 - acc: 0.9689 - val_loss: 1.1439 - val_acc: 0.8886\n","Epoch 75/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0261 - acc: 0.9763 - val_loss: 1.2625 - val_acc: 0.8796\n","Epoch 76/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0391 - acc: 0.9675 - val_loss: 1.1859 - val_acc: 0.8393\n","Epoch 77/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0924 - acc: 0.9345 - val_loss: 0.9005 - val_acc: 0.8841\n","Epoch 78/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0602 - acc: 0.9469 - val_loss: 1.3522 - val_acc: 0.8918\n","Epoch 79/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0390 - acc: 0.9649 - val_loss: 1.2338 - val_acc: 0.8713\n","Epoch 80/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0279 - acc: 0.9785 - val_loss: 1.5099 - val_acc: 0.8886\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n"," 50%|█████     | 3/6 [1:58:49<1:59:36, 2392.10s/it]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'b_size': 128, 'decay': 0.0001, 'dense1': 512, 'dense2': 128, 'drop1': 0.6, 'drop2': 0.5, 'epochs': 75, 'lr': 0.01, 'otimizer': <class 'keras.optimizers.SGD'>, 'w1': [0.25, 4]}\n","yes2x\n","Train on 6500 samples, validate on 1562 samples\n","Epoch 1/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4972 - acc: 0.1768 - val_loss: 0.4687 - val_acc: 0.1524\n","Epoch 2/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4747 - acc: 0.1569 - val_loss: 0.4693 - val_acc: 0.1524\n","Epoch 3/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4749 - acc: 0.1569 - val_loss: 0.4682 - val_acc: 0.1524\n","Epoch 4/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4734 - acc: 0.1569 - val_loss: 0.4672 - val_acc: 0.1524\n","Epoch 5/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4723 - acc: 0.1569 - val_loss: 0.4647 - val_acc: 0.1524\n","Epoch 6/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4659 - acc: 0.1569 - val_loss: 0.4557 - val_acc: 0.1524\n","Epoch 7/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4499 - acc: 0.1603 - val_loss: 0.4097 - val_acc: 0.1524\n","Epoch 8/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4221 - acc: 0.2352 - val_loss: 0.3787 - val_acc: 0.3924\n","Epoch 9/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4074 - acc: 0.3162 - val_loss: 0.3768 - val_acc: 0.5653\n","Epoch 10/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3946 - acc: 0.3840 - val_loss: 0.3634 - val_acc: 0.3764\n","Epoch 11/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3966 - acc: 0.3828 - val_loss: 0.3790 - val_acc: 0.3092\n","Epoch 12/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3852 - acc: 0.4160 - val_loss: 0.3584 - val_acc: 0.4437\n","Epoch 13/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3755 - acc: 0.4569 - val_loss: 0.3464 - val_acc: 0.5032\n","Epoch 14/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3707 - acc: 0.4457 - val_loss: 0.3563 - val_acc: 0.6005\n","Epoch 15/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3747 - acc: 0.4574 - val_loss: 0.3408 - val_acc: 0.5615\n","Epoch 16/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3645 - acc: 0.4798 - val_loss: 0.3443 - val_acc: 0.4257\n","Epoch 17/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3661 - acc: 0.4746 - val_loss: 0.3404 - val_acc: 0.5038\n","Epoch 18/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3679 - acc: 0.4662 - val_loss: 0.3414 - val_acc: 0.4802\n","Epoch 19/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3544 - acc: 0.5063 - val_loss: 0.3373 - val_acc: 0.4341\n","Epoch 20/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3478 - acc: 0.5185 - val_loss: 0.3273 - val_acc: 0.5864\n","Epoch 21/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3481 - acc: 0.5151 - val_loss: 0.3226 - val_acc: 0.5301\n","Epoch 22/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3421 - acc: 0.5402 - val_loss: 0.3189 - val_acc: 0.6012\n","Epoch 23/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3374 - acc: 0.5398 - val_loss: 0.3192 - val_acc: 0.5499\n","Epoch 24/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3356 - acc: 0.5586 - val_loss: 0.3228 - val_acc: 0.4507\n","Epoch 25/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 0.3283 - acc: 0.5574 - val_loss: 0.3240 - val_acc: 0.4840\n","Epoch 26/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3206 - acc: 0.5785 - val_loss: 0.3214 - val_acc: 0.7081\n","Epoch 27/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3178 - acc: 0.5755 - val_loss: 0.3030 - val_acc: 0.6428\n","Epoch 28/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3237 - acc: 0.5629 - val_loss: 0.3034 - val_acc: 0.5999\n","Epoch 29/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3042 - acc: 0.5971 - val_loss: 0.3007 - val_acc: 0.6601\n","Epoch 30/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2947 - acc: 0.6325 - val_loss: 0.2919 - val_acc: 0.6114\n","Epoch 31/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3017 - acc: 0.6174 - val_loss: 0.2968 - val_acc: 0.5666\n","Epoch 32/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2986 - acc: 0.6154 - val_loss: 0.2859 - val_acc: 0.7023\n","Epoch 33/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2877 - acc: 0.6502 - val_loss: 0.2813 - val_acc: 0.6607\n","Epoch 34/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2761 - acc: 0.6623 - val_loss: 0.3289 - val_acc: 0.8227\n","Epoch 35/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2824 - acc: 0.6622 - val_loss: 0.2826 - val_acc: 0.6633\n","Epoch 36/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2752 - acc: 0.6729 - val_loss: 0.3250 - val_acc: 0.5128\n","Epoch 37/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2630 - acc: 0.6905 - val_loss: 0.2901 - val_acc: 0.7029\n","Epoch 38/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2654 - acc: 0.6868 - val_loss: 0.3027 - val_acc: 0.5224\n","Epoch 39/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2600 - acc: 0.6922 - val_loss: 0.2740 - val_acc: 0.7382\n","Epoch 40/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2596 - acc: 0.7120 - val_loss: 0.2843 - val_acc: 0.7810\n","Epoch 41/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2535 - acc: 0.7175 - val_loss: 0.3032 - val_acc: 0.5384\n","Epoch 42/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2486 - acc: 0.7246 - val_loss: 0.2688 - val_acc: 0.6735\n","Epoch 43/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2437 - acc: 0.7372 - val_loss: 0.2757 - val_acc: 0.7823\n","Epoch 44/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2355 - acc: 0.7528 - val_loss: 0.2853 - val_acc: 0.7695\n","Epoch 45/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.2237 - acc: 0.7646 - val_loss: 0.2798 - val_acc: 0.7318\n","Epoch 46/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2267 - acc: 0.7583 - val_loss: 0.3508 - val_acc: 0.8572\n","Epoch 47/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2203 - acc: 0.7649 - val_loss: 0.2988 - val_acc: 0.8169\n","Epoch 48/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2183 - acc: 0.7623 - val_loss: 0.2788 - val_acc: 0.7567\n","Epoch 49/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2171 - acc: 0.7720 - val_loss: 0.3167 - val_acc: 0.8323\n","Epoch 50/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2068 - acc: 0.7746 - val_loss: 0.2966 - val_acc: 0.7311\n","Epoch 51/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.1972 - acc: 0.7898 - val_loss: 0.2993 - val_acc: 0.7606\n","Epoch 52/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1899 - acc: 0.8075 - val_loss: 0.3079 - val_acc: 0.7996\n","Epoch 53/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1898 - acc: 0.8114 - val_loss: 0.3109 - val_acc: 0.8099\n","Epoch 54/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1730 - acc: 0.8255 - val_loss: 0.3887 - val_acc: 0.5909\n","Epoch 55/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1827 - acc: 0.8106 - val_loss: 0.3405 - val_acc: 0.6741\n","Epoch 56/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1763 - acc: 0.8172 - val_loss: 0.3837 - val_acc: 0.8502\n","Epoch 57/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1759 - acc: 0.8266 - val_loss: 0.3333 - val_acc: 0.8105\n","Epoch 58/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1538 - acc: 0.8455 - val_loss: 0.3664 - val_acc: 0.8528\n","Epoch 59/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1489 - acc: 0.8542 - val_loss: 0.4289 - val_acc: 0.8841\n","Epoch 60/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1515 - acc: 0.8486 - val_loss: 0.4107 - val_acc: 0.8348\n","Epoch 61/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1347 - acc: 0.8595 - val_loss: 0.5387 - val_acc: 0.8963\n","Epoch 62/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1402 - acc: 0.8617 - val_loss: 0.5701 - val_acc: 0.8937\n","Epoch 63/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1376 - acc: 0.8683 - val_loss: 0.3882 - val_acc: 0.8406\n","Epoch 64/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1318 - acc: 0.8665 - val_loss: 0.5743 - val_acc: 0.8528\n","Epoch 65/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1375 - acc: 0.8646 - val_loss: 0.4187 - val_acc: 0.8464\n","Epoch 66/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1183 - acc: 0.8858 - val_loss: 0.5573 - val_acc: 0.8835\n","Epoch 67/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1123 - acc: 0.8915 - val_loss: 0.5431 - val_acc: 0.8873\n","Epoch 68/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1119 - acc: 0.9000 - val_loss: 0.4736 - val_acc: 0.8732\n","Epoch 69/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1131 - acc: 0.8888 - val_loss: 0.5218 - val_acc: 0.8732\n","Epoch 70/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1038 - acc: 0.9094 - val_loss: 0.6199 - val_acc: 0.8790\n","Epoch 71/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1016 - acc: 0.9043 - val_loss: 0.7793 - val_acc: 0.9046\n","Epoch 72/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0978 - acc: 0.9058 - val_loss: 0.6277 - val_acc: 0.9033\n","Epoch 73/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0896 - acc: 0.9135 - val_loss: 0.6277 - val_acc: 0.8803\n","Epoch 74/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0995 - acc: 0.9037 - val_loss: 0.5967 - val_acc: 0.8592\n","Epoch 75/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0841 - acc: 0.9202 - val_loss: 0.6485 - val_acc: 0.8726\n","Epoch 76/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0818 - acc: 0.9218 - val_loss: 0.9196 - val_acc: 0.9085\n","Epoch 77/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1030 - acc: 0.8968 - val_loss: 0.4823 - val_acc: 0.8278\n","Epoch 78/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0801 - acc: 0.9274 - val_loss: 0.6990 - val_acc: 0.8764\n","Epoch 79/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0861 - acc: 0.9234 - val_loss: 0.5539 - val_acc: 0.8688\n","Epoch 80/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0737 - acc: 0.9312 - val_loss: 0.6820 - val_acc: 0.8771\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n"," 67%|██████▋   | 4/6 [2:38:13<1:19:27, 2383.67s/it]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'b_size': 128, 'decay': 0.0001, 'dense1': 512, 'dense2': 256, 'drop1': 0.6, 'drop2': 0.5, 'epochs': 100, 'lr': 0.01, 'otimizer': <class 'keras.optimizers.SGD'>, 'w1': [0.25, 4]}\n","yes2x\n","Train on 6500 samples, validate on 1562 samples\n","Epoch 1/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4914 - acc: 0.1760 - val_loss: 0.4688 - val_acc: 0.1524\n","Epoch 2/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4738 - acc: 0.1569 - val_loss: 0.4710 - val_acc: 0.1524\n","Epoch 3/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4746 - acc: 0.1569 - val_loss: 0.4680 - val_acc: 0.1524\n","Epoch 4/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4728 - acc: 0.1569 - val_loss: 0.4667 - val_acc: 0.1524\n","Epoch 5/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.4699 - acc: 0.1569 - val_loss: 0.4622 - val_acc: 0.1524\n","Epoch 6/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4645 - acc: 0.1569 - val_loss: 0.4451 - val_acc: 0.1524\n","Epoch 7/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4419 - acc: 0.1749 - val_loss: 0.3998 - val_acc: 0.1844\n","Epoch 8/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4196 - acc: 0.2674 - val_loss: 0.3757 - val_acc: 0.3380\n","Epoch 9/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.4041 - acc: 0.3515 - val_loss: 0.3725 - val_acc: 0.3054\n","Epoch 10/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3931 - acc: 0.3754 - val_loss: 0.3560 - val_acc: 0.4417\n","Epoch 11/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3887 - acc: 0.3940 - val_loss: 0.3566 - val_acc: 0.4341\n","Epoch 12/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3868 - acc: 0.4115 - val_loss: 0.3506 - val_acc: 0.4366\n","Epoch 13/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3727 - acc: 0.4546 - val_loss: 0.3414 - val_acc: 0.5314\n","Epoch 14/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.3730 - acc: 0.4540 - val_loss: 0.3417 - val_acc: 0.5243\n","Epoch 15/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3696 - acc: 0.4702 - val_loss: 0.3369 - val_acc: 0.5256\n","Epoch 16/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3567 - acc: 0.4935 - val_loss: 0.3487 - val_acc: 0.4968\n","Epoch 17/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.3552 - acc: 0.5095 - val_loss: 0.3361 - val_acc: 0.6370\n","Epoch 18/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3565 - acc: 0.4922 - val_loss: 0.3330 - val_acc: 0.5051\n","Epoch 19/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3529 - acc: 0.5103 - val_loss: 0.3324 - val_acc: 0.4981\n","Epoch 20/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3412 - acc: 0.5338 - val_loss: 0.3340 - val_acc: 0.4501\n","Epoch 21/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3384 - acc: 0.5397 - val_loss: 0.3311 - val_acc: 0.6959\n","Epoch 22/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3316 - acc: 0.5718 - val_loss: 0.3172 - val_acc: 0.5403\n","Epoch 23/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3280 - acc: 0.5755 - val_loss: 0.3225 - val_acc: 0.7145\n","Epoch 24/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3147 - acc: 0.6052 - val_loss: 0.3512 - val_acc: 0.8047\n","Epoch 25/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3222 - acc: 0.5892 - val_loss: 0.3356 - val_acc: 0.7618\n","Epoch 26/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3083 - acc: 0.6103 - val_loss: 0.3151 - val_acc: 0.5819\n","Epoch 27/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.3069 - acc: 0.6169 - val_loss: 0.3103 - val_acc: 0.5403\n","Epoch 28/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.2977 - acc: 0.6312 - val_loss: 0.2947 - val_acc: 0.6569\n","Epoch 29/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2952 - acc: 0.6488 - val_loss: 0.3362 - val_acc: 0.4270\n","Epoch 30/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2888 - acc: 0.6428 - val_loss: 0.3026 - val_acc: 0.5640\n","Epoch 31/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2838 - acc: 0.6551 - val_loss: 0.2931 - val_acc: 0.6172\n","Epoch 32/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2820 - acc: 0.6665 - val_loss: 0.3056 - val_acc: 0.6780\n","Epoch 33/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2686 - acc: 0.6975 - val_loss: 0.2898 - val_acc: 0.6793\n","Epoch 34/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2654 - acc: 0.6838 - val_loss: 0.2883 - val_acc: 0.7490\n","Epoch 35/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2634 - acc: 0.6920 - val_loss: 0.2786 - val_acc: 0.6645\n","Epoch 36/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2502 - acc: 0.7154 - val_loss: 0.3711 - val_acc: 0.8636\n","Epoch 37/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2414 - acc: 0.7325 - val_loss: 0.2960 - val_acc: 0.7650\n","Epoch 38/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2359 - acc: 0.7312 - val_loss: 0.3159 - val_acc: 0.6396\n","Epoch 39/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2405 - acc: 0.7291 - val_loss: 0.2938 - val_acc: 0.7708\n","Epoch 40/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2293 - acc: 0.7518 - val_loss: 0.2946 - val_acc: 0.7625\n","Epoch 41/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2246 - acc: 0.7395 - val_loss: 0.3128 - val_acc: 0.7798\n","Epoch 42/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2221 - acc: 0.7614 - val_loss: 0.3007 - val_acc: 0.7478\n","Epoch 43/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2038 - acc: 0.7868 - val_loss: 0.3026 - val_acc: 0.7798\n","Epoch 44/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.2054 - acc: 0.7737 - val_loss: 0.2931 - val_acc: 0.7836\n","Epoch 45/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1958 - acc: 0.7943 - val_loss: 0.3064 - val_acc: 0.7971\n","Epoch 46/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1853 - acc: 0.8029 - val_loss: 0.3076 - val_acc: 0.7330\n","Epoch 47/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1783 - acc: 0.8157 - val_loss: 0.3657 - val_acc: 0.7638\n","Epoch 48/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1863 - acc: 0.8072 - val_loss: 0.3516 - val_acc: 0.7996\n","Epoch 49/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1652 - acc: 0.8291 - val_loss: 0.4180 - val_acc: 0.7682\n","Epoch 50/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1638 - acc: 0.8249 - val_loss: 0.3349 - val_acc: 0.8182\n","Epoch 51/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1662 - acc: 0.8262 - val_loss: 0.4446 - val_acc: 0.8630\n","Epoch 52/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1541 - acc: 0.8471 - val_loss: 0.4685 - val_acc: 0.8099\n","Epoch 53/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1482 - acc: 0.8492 - val_loss: 0.3783 - val_acc: 0.8047\n","Epoch 54/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1341 - acc: 0.8635 - val_loss: 0.4214 - val_acc: 0.8233\n","Epoch 55/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1311 - acc: 0.8706 - val_loss: 0.4199 - val_acc: 0.8227\n","Epoch 56/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1296 - acc: 0.8660 - val_loss: 0.5491 - val_acc: 0.8675\n","Epoch 57/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1120 - acc: 0.8905 - val_loss: 0.4625 - val_acc: 0.7887\n","Epoch 58/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1227 - acc: 0.8726 - val_loss: 0.3895 - val_acc: 0.7926\n","Epoch 59/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1151 - acc: 0.8877 - val_loss: 0.4665 - val_acc: 0.7503\n","Epoch 60/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1329 - acc: 0.8600 - val_loss: 0.6127 - val_acc: 0.8950\n","Epoch 61/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.1042 - acc: 0.9006 - val_loss: 0.6382 - val_acc: 0.8803\n","Epoch 62/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0976 - acc: 0.9060 - val_loss: 0.6029 - val_acc: 0.8745\n","Epoch 63/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.1054 - acc: 0.8937 - val_loss: 0.6228 - val_acc: 0.8662\n","Epoch 64/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0806 - acc: 0.9192 - val_loss: 0.5891 - val_acc: 0.8726\n","Epoch 65/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0918 - acc: 0.9125 - val_loss: 0.5884 - val_acc: 0.8662\n","Epoch 66/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0853 - acc: 0.9174 - val_loss: 0.7464 - val_acc: 0.8969\n","Epoch 67/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0810 - acc: 0.9225 - val_loss: 0.7328 - val_acc: 0.8553\n","Epoch 68/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0708 - acc: 0.9320 - val_loss: 0.7942 - val_acc: 0.8777\n","Epoch 69/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0914 - acc: 0.9097 - val_loss: 0.6491 - val_acc: 0.8688\n","Epoch 70/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0642 - acc: 0.9375 - val_loss: 0.7011 - val_acc: 0.8739\n","Epoch 71/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0792 - acc: 0.9260 - val_loss: 0.8228 - val_acc: 0.8886\n","Epoch 72/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0600 - acc: 0.9431 - val_loss: 0.7840 - val_acc: 0.8720\n","Epoch 73/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0575 - acc: 0.9468 - val_loss: 0.7681 - val_acc: 0.8643\n","Epoch 74/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0816 - acc: 0.9235 - val_loss: 0.8451 - val_acc: 0.8950\n","Epoch 75/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0679 - acc: 0.9372 - val_loss: 1.0546 - val_acc: 0.8969\n","Epoch 76/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0514 - acc: 0.9537 - val_loss: 0.7735 - val_acc: 0.8777\n","Epoch 77/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0551 - acc: 0.9454 - val_loss: 0.8889 - val_acc: 0.8899\n","Epoch 78/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 0.0645 - acc: 0.9425 - val_loss: 0.8418 - val_acc: 0.8835\n","Epoch 79/80\n","6500/6500 [==============================] - 29s 4ms/step - loss: 0.0481 - acc: 0.9542 - val_loss: 0.8919 - val_acc: 0.8880\n","Epoch 80/80\n","6500/6500 [==============================] - 29s 5ms/step - loss: 0.0625 - acc: 0.9452 - val_loss: 0.8877 - val_acc: 0.8944\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n"," 83%|████████▎ | 5/6 [3:17:35<39:36, 2376.96s/it]  \u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'b_size': 64, 'decay': 1e-06, 'dense1': 512, 'dense2': 128, 'drop1': 0.6, 'drop2': 0.5, 'epochs': 75, 'lr': 0.01, 'otimizer': <class 'keras.optimizers.Adam'>, 'w1': [0.25, 4]}\n","yes\n","Train on 6500 samples, validate on 1562 samples\n","Epoch 1/80\n","6500/6500 [==============================] - 32s 5ms/step - loss: 3.3714 - acc: 0.1595 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 2/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 3/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 4/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 5/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 6/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 7/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 8/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 9/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 10/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 11/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 12/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 13/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 14/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 15/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 16/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 17/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 18/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 19/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 20/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 21/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 22/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 23/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 24/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 25/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 26/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 27/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 28/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 29/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 30/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 31/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 32/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 33/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 34/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 35/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 36/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 37/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 38/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 39/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 40/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 41/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 42/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 43/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 44/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 45/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 46/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 47/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 48/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 49/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 50/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 51/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 52/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 53/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 54/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 55/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 56/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 57/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 58/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 59/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 60/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 61/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 62/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 63/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 64/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 65/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 66/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 67/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 68/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 69/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 70/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 71/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 72/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 73/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 74/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 75/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 76/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 77/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 78/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 79/80\n","6500/6500 [==============================] - 31s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n","Epoch 80/80\n","6500/6500 [==============================] - 30s 5ms/step - loss: 3.3972 - acc: 0.1569 - val_loss: 3.4156 - val_acc: 0.1524\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","100%|██████████| 6/6 [3:58:26<00:00, 2399.27s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\u001b[A\u001b[A\u001b[A"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"I61Ngo5CDslJ","colab_type":"code","colab":{}},"source":["#Resume of experiments: training and validation accuracy/loss\n","# PS: experiment no. 4 were replaced by an individual experiment (option)\n","\n","t.data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0pNwr5OGu_B8","colab_type":"text"},"source":["CHOOSE THE BEST MODEL 2\n","\n","--Define and replace the hyperparameters values for each of the 6 experiments (manually) and re-train each configuration.\n","\n","--Save the best weights: for the highest validation accuracy obtained before validation loss starts to increse."]},{"cell_type":"code","metadata":{"id":"O0WEYv8xvBOk","colab_type":"code","colab":{}},"source":["from keras.optimizers import SGD , RMSprop , Adadelta , Adam\n","from keras.layers.normalization import BatchNormalization\n","from sklearn.metrics import recall_score\n","\n","def weighted_pneumo_model():\n","\n","  model = Sequential()\n","  model.add(Conv2D(16, (3, 3), activation='relu', padding=\"same\", input_shape=(200,200,3), data_format='channels_last'))\n","  model.add(Conv2D(16, (3, 3), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  #model.add(BatchNormalization())\n","\n","  model.add(Conv2D(32, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(Conv2D(32, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Conv2D(64, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(Conv2D(64, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Conv2D(128, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(Conv2D(128, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Conv2D(256, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(Conv2D(256, (2, 2), activation='relu', padding=\"same\"))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))    \n","\n","  model.add(Flatten())        \n","\n","\n","  model.add(Dense(256, activation='relu'))\n","  \n","  model.add(Dropout(0.6))\n","  model.add(Dense(128, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(2 , activation='softmax'))\n","\n","  from sklearn import metrics\n","  from keras import backend as K\n","  from tensorflow.keras.metrics import Recall\n","  from keras import optimizers\n","\n","  w=np.array([0.25,4])\n","  w_loss=weighted_categorical_crossentropy(w)\n","  \n","  model.compile(loss=w_loss,\n","                    # here we add a regulizer normalization function from Talos\n","                    optimizer=optimizers.sgd(lr=0.01,decay=1e-06, momentum=0.9, nesterov=True),\n","                    metrics=['acc'])\n","\n","\n","  history = model.fit(X_train, y_train,\n","                        batch_size=128,\n","                        epochs=150,\n","                        validation_data=(X_val, y_val),\n","                      callbacks=[earlystop]\n","                        )\n","\n","  tf.get_default_graph()\n","    # finally we have to make sure that history object and model are returned\n","  return  model, history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VF-CUEa5fsNh","colab_type":"text"},"source":["MODEL 2 PERFORMANCE ON VALIDATION SET XP1'"]},{"cell_type":"code","metadata":{"id":"AwOZCGKZv823","colab_type":"code","colab":{}},"source":["#load model2 with the replaced hyperparameters values (with each experiment configuration)\n","model_2=weighted_pneumo_model()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LHkwchmZygfp","colab_type":"code","colab":{}},"source":["#only get the model architecture and parameters\n","bestmodel=model_2[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qOWGhddCyvyP","colab_type":"code","colab":{}},"source":["##load the saved weights of each experiment ##load the saved weights of each experiment (IT IS IMPORTANT THAT \"model_2\" has the same configuration as the experiment in loading. Otherwise it will give errors in the weights)\n","bestmodel.load_weights('/content/drive/My Drive/identify_pneumonia-master/model_2_weights/0.hdf5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wdrMvKhywc8","colab_type":"code","colab":{}},"source":["#load the validation arrays and labels\n","X_test=np.load('/content/drive/My Drive/identify_pneumonia-master/colab_Data/x_val_200.npy')\n","y_test=np.load('/content/drive/My Drive/identify_pneumonia-master/colab_Data/y_val_200.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjCssJFlyyPb","colab_type":"code","colab":{}},"source":["##reshape if there is an error on input shape\n","\n","X_test=X_test.reshape(500,200,200,3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PlOeT_XS5Pk","colab_type":"code","colab":{}},"source":["##Results on validation data\n","\n","from mlxtend.plotting import plot_confusion_matrix\n","\n","from sklearn.metrics import confusion_matrix\n","y_pred = bestmodel.predict(X_val)\n","\n","thresholders=[]  \n","for x in range(len(y_pred)):\n","  if y_pred[x][1]>0.5:   #define threshold range (optional)\n","    thresholders.append(1)\n","  else:\n","    thresholders.append(0)\n","\n","thr_pred=np.asarray(thresholders)\n","\n","y_pred = np.argmax(y_pred,axis = 1)\n","y_true = np.argmax(y_val, axis=1)\n","\n","CM = confusion_matrix(y_true, y_pred)\n","fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(3, 3))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7i6bG7KnM4E","colab_type":"text"},"source":["FROM THE VALIDATION PREDICTION RESULTS ABOVE WE CHOOSE EXPERIMENT 1 AS BEST MODEL 2!!!\n","\n","--Load best model weights for each experiment"]},{"cell_type":"code","metadata":{"id":"S_z_HMpBjgDX","colab_type":"code","colab":{}},"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_true, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sf0QMOGneVx9","colab_type":"code","colab":{}},"source":["##Plot the model behavior\n","history=model_2[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJ7WtTkX7LCP","colab_type":"code","outputId":"342b61eb-23b2-4f5a-c7a5-bf32ab6cbf33","executionInfo":{"status":"ok","timestamp":1573082621181,"user_tz":0,"elapsed":1542,"user":{"displayName":"Nuno Carlos","photoUrl":"","userId":"16472806472979258588"}},"colab":{"base_uri":"https://localhost:8080/","height":573}},"source":["# Plot training & validation accuracy values\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPMNXw1+vsAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICRJTV8ml4hFUCuy8wAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}